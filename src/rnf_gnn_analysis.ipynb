{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aefdc41e-5a09-490e-afa5-14af496881ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import bnlearn as bn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "from dgl import DGLGraph\n",
    "from modules import GAT_RNF\n",
    "\n",
    "from torch_geometric.explain import AttentionExplainer, ModelConfig, ExplainerConfig, Explainer, GNNExplainer\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "41f1bfbb-6d67-4d2b-9c0c-84712d333146",
   "metadata": {},
   "outputs": [],
   "source": [
    "def undirected_SHD(g1, g2):\n",
    "    g1 = np.tril(((g1 + g1.T) > 0).astype(int))\n",
    "    g2 = np.tril(((g2 + g2.T) > 0).astype(int))\n",
    "    return np.sum(np.abs(g1-g2))\n",
    "\n",
    "def comb_SHD(g1, g2):\n",
    "    return np.sum(np.abs(g1 - g2)), undirected_SHD(g1,g2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "495ab70a-5958-4d35-b14f-d1cab379202c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4bb4c699-71b2-4da4-a46e-32e2870494a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME=\"insurance\"\n",
    "NUM_SAMPLES=10000\n",
    "RNF_DIM=50\n",
    "RNF_INTERMEDIATE_DIM=25\n",
    "RNF_INIT_METHOD=\"xavier_uniform\"\n",
    "NUM_HEADS=3\n",
    "NUM_EPOCHS=50\n",
    "LEARNING_RATE=0.001\n",
    "BATCH_SIZE=64\n",
    "EXPERIMENT_ID=\"asia_v1\"\n",
    "NUM_EPOCHS=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "dba1efa1-7154-4c6e-a019-c645c481d92a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[bnlearn] >Import <BIFs/insurance.bif>\n",
      "[bnlearn] >Loading bif file <BIFs/insurance.bif>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[setgraphviz] >WARNING> Probability values don't exactly sum to 1. Differ by: -2.220446049250313e-16. Adjusting values.\n",
      "[setgraphviz] >WARNING> Probability values don't exactly sum to 1. Differ by: 2.220446049250313e-16. Adjusting values.\n",
      "[setgraphviz] >WARNING> Probability values don't exactly sum to 1. Differ by: -2.220446049250313e-16. Adjusting values.\n",
      "[setgraphviz] >WARNING> Probability values don't exactly sum to 1. Differ by: -2.220446049250313e-16. Adjusting values.\n",
      "[setgraphviz] >WARNING> Probability values don't exactly sum to 1. Differ by: -2.220446049250313e-16. Adjusting values.\n",
      "[setgraphviz] >WARNING> Probability values don't exactly sum to 1. Differ by: -2.220446049250313e-16. Adjusting values.\n",
      "[setgraphviz] >WARNING> Probability values don't exactly sum to 1. Differ by: -2.220446049250313e-16. Adjusting values.\n",
      "[setgraphviz] >WARNING> Probability values don't exactly sum to 1. Differ by: 1.1102230246251565e-16. Adjusting values.\n",
      "[setgraphviz] >WARNING> Probability values don't exactly sum to 1. Differ by: -2.220446049250313e-16. Adjusting values.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[bnlearn] >Check whether CPDs sum up to one.\n",
      "[bnlearn] >CPD [Accident] does not add up to 1 but is: [[[1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]]]\n",
      "[bnlearn] >CPD [CarValue] does not add up to 1 but is: [[[1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]]]\n",
      "[bnlearn] >CPD [DrivHist] does not add up to 1 but is: [[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]]\n",
      "[bnlearn] >CPD [DrivingSkill] does not add up to 1 but is: [[1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]]\n",
      "[bnlearn] >CPD [HomeBase] does not add up to 1 but is: [[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]]\n",
      "[bnlearn] >CPD [MedCost] does not add up to 1 but is: [[[1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1.]]]\n",
      "[bnlearn] >CPD [OtherCarCost] does not add up to 1 but is: [[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "[bnlearn] >CPD [RiskAversion] does not add up to 1 but is: [[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#### Defining the paths ####\n",
    "dataset_path = f\"BIFs/{DATASET_NAME}.bif\"\n",
    "\n",
    "save_dir = f\"results/{EXPERIMENT_ID}\"\n",
    "#### Building the dataset ####\n",
    "causal_model = bn.import_DAG(dataset_path)\n",
    "df_samples = bn.sampling(causal_model, n=NUM_SAMPLES)\n",
    "nodes_orig = list(causal_model['adjmat'].columns)\n",
    "num_nodes = len(nodes_orig)\n",
    "\n",
    "target_symmetric_adj = (causal_model['adjmat'] + causal_model['adjmat'].T).to_numpy()\n",
    "\n",
    "node_features = torch.from_numpy(df_samples.to_numpy() - 0.5).type(torch.float32).unsqueeze(2)\n",
    "\n",
    "#### Defining functionality ####\n",
    "def build_graph(features, num_nodes, src_nodes, dst_nodes):\n",
    "    g = DGLGraph()\n",
    "    g.add_nodes(num_nodes)\n",
    "    g.ndata['features'] = features\n",
    "    g.add_edges(src_nodes, dst_nodes)\n",
    "    return g\n",
    "\n",
    "def collate_fn(data):\n",
    "    return data\n",
    "    \n",
    "#### Preparing for training ####\n",
    "initial_adj = torch.ones((num_nodes, num_nodes)) - torch.eye(num_nodes)\n",
    "src_nodes, dst_nodes = torch.nonzero(initial_adj).T\n",
    "\n",
    "graph_dataset = [build_graph(features, num_nodes, src_nodes, dst_nodes) for features in node_features]\n",
    "\n",
    "loader = DataLoader(graph_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "#### Main Operations\n",
    "model = GAT_RNF(num_nodes=len(nodes_orig), rnf_dim=RNF_DIM, rnf_intermediate_dim=RNF_INTERMEDIATE_DIM, rnf_init_method=RNF_INIT_METHOD, num_heads=NUM_HEADS)\n",
    "\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "2378138e-3a3a-4274-b9d7-222ffcc9a65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(loader, model, optimizer, loss_fn):\n",
    "    for g_list in loader:\n",
    "        optimizer.zero_grad()\n",
    "        target_ls, pred_ls = [], []\n",
    "        for g in g_list:\n",
    "            target_ls.append(g.ndata['features'])\n",
    "            out = model(g, g.ndata['features'])\n",
    "            pred_ls.append(out)\n",
    "        loss = loss_fn(torch.stack(tuple(out)), torch.stack(tuple(target_ls)))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "01c273d5-e4cc-4958-84b4-7e223440d566",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Epoch 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[134], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,NUM_EPOCHS\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved_model/gat_rnf_insurance.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[133], line 7\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(loader, model, optimizer, loss_fn)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m g_list:\n\u001b[1;32m      6\u001b[0m     target_ls\u001b[38;5;241m.\u001b[39mappend(g\u001b[38;5;241m.\u001b[39mndata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 7\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     pred_ls\u001b[38;5;241m.\u001b[39mappend(out)\n\u001b[1;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(torch\u001b[38;5;241m.\u001b[39mstack(\u001b[38;5;28mtuple\u001b[39m(out)), torch\u001b[38;5;241m.\u001b[39mstack(\u001b[38;5;28mtuple\u001b[39m(target_ls)))\n",
      "File \u001b[0;32m~/miniforge3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Academic/Cambridge/L65/Project/src/modules.py:373\u001b[0m, in \u001b[0;36mGAT_RNF.forward\u001b[0;34m(self, g, x, get_attention)\u001b[0m\n\u001b[1;32m    371\u001b[0m dist \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mNormal(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnf_mean, nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftplus(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnf_std) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.0001\u001b[39m)\n\u001b[1;32m    372\u001b[0m rnf_features \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mrsample()\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgatconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrnf_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_attention\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_attention\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m get_attention:\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmean(out[\u001b[38;5;241m0\u001b[39m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), out[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Academic/Cambridge/L65/Project/src/layers.py:131\u001b[0m, in \u001b[0;36mRNF_GATConv.forward\u001b[0;34m(self, graph, feat, target, edge_weight, get_attention)\u001b[0m\n\u001b[1;32m    127\u001b[0m er \u001b[38;5;241m=\u001b[39m (feat_dst_attn \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_r)\n\u001b[1;32m    129\u001b[0m src_indices, dst_indices \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39medges()\n\u001b[0;32m--> 131\u001b[0m projected_pre_attention \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_project_attn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleaky_relu(\u001b[43mth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mel\u001b[49m\u001b[43m[\u001b[49m\u001b[43msrc_indices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mer\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdst_indices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m))\n\u001b[1;32m    132\u001b[0m a \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_drop(edge_softmax(graph, projected_pre_attention)), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m#### Calculating tanh coefficients ####\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch_id in range(1,NUM_EPOCHS+1):\n",
    "    print(f'Epoch {epoch_id}')\n",
    "    train_epoch(loader, model, optimizer, loss_fn)\n",
    "torch.save(model.state_dict(), \"saved_model/gat_rnf_insurance.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a242c1c2-ffe8-49a9-9b94-daa93d00a9e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"saved_model/gat_rnf_insurance.pt\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f5b6a2-e937-4647-a19e-8b701996435c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "atts_l = []\n",
    "for g_list in loader:\n",
    "    optimizer.zero_grad()\n",
    "    target_ls, pred_ls = [], []\n",
    "    for g in g_list:\n",
    "        target_ls.append(g.ndata['features'])\n",
    "        out = model(g, g.ndata['features'], get_attention=True)\n",
    "        pred_ls.append(out[0])\n",
    "        atts_l.append(out[1].detach().numpy())\n",
    "    loss = loss_fn(torch.stack(tuple(out[0])), torch.stack(tuple(target_ls)))\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f307ed66-78bb-42fd-9eb2-df73f2f557ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "atts = np.array(atts_l).reshape(NUM_SAMPLES, -1)\n",
    "atts = np.mean(atts, axis=0).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99f09ed-fef4-4d20-9824-f67d61b2d9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = len(nodes_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fbbcb1-11da-4056-b021-7a9f8c4dee36",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_adj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75031d2-ab57-4625-ac31-626f1e5a7442",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(atts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f5c265-5914-4d81-918f-56605ace83ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_adj = np.zeros_like(initial_adj)\n",
    "thresh = np.mean(atts) + 1.7 * np.std(atts)\n",
    "edges_list = np.where(initial_adj>0)\n",
    "for i in range(len(np.where(initial_adj>0)[0])):\n",
    "    e_i = (edges_list[0][i], edges_list[1][i])\n",
    "    if atts[i] > thresh:\n",
    "        new_adj[e_i[0]][e_i[1]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2944c7d9-cd84-40d2-a113-2e0d27f1cee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afabd4b-1501-410e-8ec6-3a7620ab53b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cbfd38-f352-4920-a8e7-6542fdb676e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nx.DiGraph(new_adj, nodes=nodes_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a1e9ee-1e40-4b23-b074-3d934d882a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = nx.circular_layout(g)\n",
    "nx.draw_networkx(g, pos, labels = {\n",
    "    i:nodes_orig[i] for i in range(len(g.nodes))\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672a457d-d786-4ddf-8de9-eb77a8a93f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = nx.adjacency_matrix(g)\n",
    "shd = np.sum(np.abs(causal_model['adjmat'].astype(int).to_numpy() - adj.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d1fc64-87d3-48dc-bfc2-85bbe2e471d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_SHD(adj.toarray(), causal_model['adjmat'].astype(int).to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf90fd3-b015-4646-bd52-be9527a0d0c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
